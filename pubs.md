---
layout: page
title: Pubs
---

<!-- Follow <a href="https://medium.com/@djpardis/" target="_blank">my blog</a> to learn about upcoming posts. -->

### Management essays

- <em><a href="https://medium.com/@djpardis/models-for-integrating-data-science-teams-within-organizations-7c5afa032ebd" target="_blank">Models for integrating data science teams within organizations: A comparative analysis</a></em>
<br/>
July 31, 2019.
<br/>
  <em>Citations and coverage.</em> <a href="https://onlinedatasciencemasters.virginia.edu/blog/need-for-interdisciplinary-data-science/" target="_blank">University of Virgina Data Science</a>, <a href="https://www.97ways.com/thelist/8-sit-with-your-stakeholders" target="_blank">97 Ways (Matt Wright)</a>, <a href="https://us20.campaign-archive.com/?e=&u=8974b971ec317d8a98dbbf292&id=05f0f9e91a" target="_blank">Projects to Know (Amplify Partners — Sarah Catanzaro)</a>, <a href="http://roundup.fishtownanalytics.com/issues/survival-analysis-better-presto-pinterest-dagster-data-science-in-organizations-a-two-fer-dsr-194-193857" target="_blank">The Data Science Roundup (Fishtown Analytics — Tristan Handy)</a>, <a href="https://vicki.substack.com/p/selling-data-science" target="_blank">Normcore Tech (Vicki Boykis)</a>, <a href="https://femstreet.substack.com/p/-parenthood-and-entrepreneurship-19-08-04" target="_blank">Femstreet (Sarah N<span>&#246;</span>ckel)</a>, <a href="http://lineardigressions.com/episodes/2019/8/25/organizational-models-for-data-scientists" target="_blank">Linear Digressions</a>, <a href="https://analyticaliq.com/data-science-staffing/" target="_blank">Analytical IQ (Adam Lorton)</a>, <a href="https://hex.tech/blog/data-team-roi">Hex Technologies Blog (Barry McCardel)</a>, <a href="https://fall2019.fullstackdeeplearning.com/course-content/where-to-go-next" target="_blank">Full Stack Deep Learning</a>, <a href="https://www.getrevue.co/profile/shashank/issues/the-ml-times-issue-14-192472" target="_blank">The ML Times</a>, <a href="https://dispatch.nibble.ai/issues/nibble-ai-weekly-issue-23-making-data-science-more-useful-deploying-ai-without-technical-debt-191252" target="_blank">nibble dispatch</a>, <a href="https://leanpub.com/dshiring" target="_blank">Hiring Data Scientists and Machine Learning Engineers: A Practical Guide (a book by Roy Keyes)</a>.

- <em><a href="https://medium.com/@djpardis/recommendations-for-data-science-team-sizing-and-allocation-strategy-a38f943638e5" target="_blank">Data science team sizing and allocation: An algorithm</a></em>
<br/>
July 29, 2019.

- <em><a href="https://djpardis.medium.com/sfelc-summit-2019-5a5b2ce91346" target="_blank">SF Engineering Leadership Community Summit 2019: Some notes from the gathering</a></em>
<br/>
January 28, 2019.

- <em><a href="https://medium.com/@djpardis/management-and-coaching-best-practices-as-a-list-of-n-things-7a6d9c7f0fa5" target="_blank">Management best practices: A list of 20 things</a></em>
<br/>
December 18, 2018.

- <em><a href="https://medium.com/@djpardis/q-a-with-steven-sinofsky-at-twitter-hq-a658ca5db953" target="_blank">Q&A with Steven Sinofsky at Twitter HQ: Developing cross-functional teams</a></em>
<br/>
November 16, 2018.

### ML panels

- <em>Building teams and culture that support ML innovation</em>
  <br/>with <a href="https://www.linkedin.com/in/ziad-asghar-794404/" target="_blank">Ziad Asghar</a> and <a href="https://www.linkedin.com/in/ameenkazerouni/" target="_blank">Ameen Kazerouni</a>, moderated by <a href="https://www.linkedin.com/in/samcharrington/" target="_blank">Sam Charrington</a>
  <br/>
  <a href="https://twimlcon.com/sessions/building-teams-and-culture-that-support-ml-innovation/">TWIMLcon</a>, January 22, 2021.
  <br/>
  <p><i>Abstract.</i>
  Traditional approaches to managing technical projects can be at odds with achieving success with machine learning. In this session, we discuss how ML and AI executives can build effective teams, support them with the right processes and tools, and shift the broader organizational culture in ways that reinforce innovation in machine learning.</p>

- <em>Making an impact in data science: when traditional methods fail</em>
  <br/>with <a href="https://www.linkedin.com/in/ziad-asghar-794404/" target="_blank">Eric Glover</a>, <a href="https://www.linkedin.com/in/ziad-asghar-794404/" target="_blank">Halim Abbas</a>, <a href="https://www.linkedin.com/in/ziad-asghar-794404/" target="_blank">Kevin Stumpf</a>, and <a href="https://www.linkedin.com/in/ameenkazerouni/" target="_blank">Sean McPherson</a>
  <br/>
  <a href="https://www.eventbrite.com/e/branch-data-science-meetup-tickets-93123429685#">Branch Data Science Meetup</a>, February 27, 2020.
  <br/>
  <p><i>Abstract.</i> In this meetup, we hear about data science projects that succeeded in spite of the limitations of existing methodology.</p>

- <em>Culture & organization for effective ML at scale</em>
  <br/>with <a href="https://www.linkedin.com/in/ecolson/" target="_blank">Eric Colson</a> and <a href="https://www.linkedin.com/in/jennifer-prendki/" target="_blank">Jennifer Prendki</a>, moderated by <a href="https://www.linkedin.com/in/maribellopez/" target="_blank">Maribel Lopez</a>
  <br/>
  <a href="https://tmt.knect365.com/ai-summit-san-francisco/speakers/pardis-noorzad/">TWIMLcon</a>, Sep 27, 2019.
  <br/>
  <p><i>Abstract.</i> Hear from people that have experienced startups and large corporations in a range of industries reveal tips to work faster, more efficiently, and create an org-wide culture that supports effective ML.</p>

### ML talks

- <em>Data Science for tech-enabled healthcare</em>
  <br/>with <a href="https://www.linkedin.com/in/rismakov/" target="_blank">Rebekkah Ismakov</a>
  <br/>
  <a title="AI Summit" href="https://tmt.knect365.com/ai-summit-san-francisco/speakers/pardis-noorzad/">The AI Summit</a>, October 1, 2020.
  <br/>
  [<a href="https://www.youtube.com/watch?v=CQHwLWMQFDk" target="_blank">video</a>, <a href="https://medium.com/carbon-blog/covid-19-risk-assessment-simulation-model-684fc27d5019" target="_blank">blog post</a>, <a href="https://covidclinicaldata.org/" target="_blank">data</a>, <a href="https://twitter.com/erenbali/status/1261083321158770689?s=20" target=_blank>discussion</a>]
  <!-- <br/>[<a href="https://www.youtube.com/watch?v=CQHwLWMQFDk" target="_blank">conference recording</a>, <a href="https://www.pscp.tv/w/1mnxeQdvAZqxX" target="_blank">pre-recording</a>] -->
  <p><i>Abstract.</i> The first part of the talk is an overview of the Data Science team roadmap and infrastructure decisions, with a tour of the clinical decision support system and <a href="http://covidclinicaldata.org/" target="_blank">covidclinicaldata.org</a>. The second part is a review of our efforts for the <a href="https://carbonhealth.com/covid-ready" target="_blank">COVID-Ready</a> program. We report on recommendations that can be made to employers, based on simulations surfacing how testing cadence and other policies affect outbreaks in the workplace.</p>

- <em>Modeling the Facebook social network: The memoryless GEO-P graph model</em>
  <br/>
  <a title="SOGMSC '14'" href="https://mathstat.uoguelph.ca/graduate/sogmsc">SOGMSC</a>, May 21, 2014.
  <br/>
  [<a href="/files/modeling_the_facebook_social_network.pdf" target="_blank">slides</a>]
  <p><i>Abstract.</i> Online social networks are ubiquitous graphs. To test algorithms that scale with the size and order of these networks, we require synthetic samples. In this talk, we go over several methods for generating random graphs representative of online social networks. We are especially interested in the M-GEOP model (<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0106052" target="_blank">Bonato et al., 2014</a>), and in assessing the fit of these models to the Facebook dataset.</p>

- <em>Efficient classification based on sparse regression</em>
  <br/>
  <a title="Amirkabir University of Technology" href="http://aut.ac.ir/aut/" target="_blank">AUT</a>, July 17, 2012.
  <br/>
  [<a href="/files/defense_slides.pdf" target="_blank">slides</a>]
  <p><i>Abstract.</i> Master's thesis defense slides.</p>

  <!-- In this thesis, we have shown that ℓ1-regularized square loss minimization for classification is a success, both computationally and statistically. We have also shown that ℓ1-regularized square loss minimization for reconstruction is not worth it. Simpler methods like kNN classification and WkNNR are at least as good. We propose four areas that hold promise for future investigation. -->

- <em>SPARROW: SPARse appROximation Weighted regression</em>
  <br/>
  <a title="University of Montreal" href="http://www.iro.umontreal.ca/?lang=en" target="_blank">UdeM</a>, March 12, 2012 and <a title="Sharif University of Technology" href="http://www.en.sharif.edu/" target="_blank">SUT</a>, February 22, 2012.
  <br/>
  [<a href="/files/sparse_approximation_weighted_regression.pdf" target="_blank">slides</a>]
  <p><i>Abstract.</i> We propose sparse approximation weighted regression (SPARROW), a nonparametric method of regression that takes advantage of the sparse linear approximation of a query point. SPARROW employs weights based on sparse approximation in the context of locally constant, locally linear, and locally quadratic regression to generate better estimates than for e.g.,  k-nearest neighbor regression and more generally, kernel-weighted local polynomial regression. Our experimental results show that SPARROW performs competitively.</p>

- <em>Sparse coding and dictionary learning</em>
  <br/>
  <a title="Sharif University of Technology" href="http://www.sharif.ir/en/" target="_blank">SUT</a>, October 5, 2011.
  <br/>
  [<a href="/files/sparse_coding_and_dictionary_learning.pdf" target="_blank">slides</a>]
  <p><i>Abstract.</i> Sparse coding is achieved by solving an under-determined system of linear equations under sparsity constraints. We briefly look at several algorithms that solve the resulting optimization problem (exactly or approximately). We then see how this optimization principle can be applied in both a supervised and unsupervised context: multiclass classification and feature learning, respectively. Next, we talk about dictionary learning and some of its well-known instances. Applications of dictionary learning include image denoising and inpainting.</p>

- <em>Feature learning with deep networks for image classification</em><br/>
  <a title="Sharif University of Technology" href="http://www.sharif.ir/en/" target="_blank">SUT</a>, May 18, 2011.
  <br/>
  [<a href="/files/feature_learning_with_deep_networks_for_image_classification.pdf" target="_blank">slides</a>]
  <p><i>Abstract.</i> An image can be represented at different levels, starting from pixels, going on to edges, to parts, to objects, and beyond. Over the years, many attempts have been made at engineering useful descriptors that are able to extract low-to-high level features from images. But what if we could make this process automatic? What if we could "learn" to detect layer after layer of features of increasing abstraction and complexity? After all, it would be impossible for us to foresee and hard-code all the kinds of invariances necessary to build features for our ever more complicated tasks. In this talk, we go over several unsupervised feature learning methods that have been in the making since 2006.</p>

- <em>Computational learning theory</em>
  <br/>
  <a title="Amirkabir University of Technology" href="http://aut.ac.ir/" target="_blank">AUT</a>, April 26, 2011.
  <br/>
  [<a href="/files/computational_learning_theory.pdf" target="_blank">slides</a>]
  <p align="justify"><i>Details.</i> This is a brief tutorial on learning theory for a machine learning class.</p>

- <em>Parametric density estimation using GMMs</em><br/>
  <a title="Amirkabir University of Technology" href="http://aut.ac.ir/" target="_blank">AUT</a>, February 1, 2011.
  <br/>
  [<a href="/files/parametric_density_estimation_using_gaussian_mixture_models.pdf" target="_blank">slides</a>]
  <p><i>Details.</i> This is a brief tutorial on applying the EM algorithm for estimating the parameters of a Gaussian mixture model.</p>

- <em>High dimensional data and dimensionality reduction</em><br/>
  <a title="Institute for Research in Fundamental Sciences" href="http://www.ipm.ac.ir/" target="_blank">IPM</a>, November 4, 2010.
  <br/>
  [<a href="/files/high-dimensional_data_and_dimensionality_reduction.pdf" target="_blank">slides</a>]
  <p><i>Abstract.</i> Apart from raising computational costs, high-dimensional data behave in counterintuitive ways. In this seminar, we talk about why in some situations, more features fail to result in increased accuracy in clustering and classification tasks. To deal with the "curses of dimensionality", many dimensionality reduction (DR) methods have been proposed. These methods map the data points to a lower-dimensional space, while preserving the important properties of the data in its original space. We go over one linear and two nonlinear DR methods. Then, through some examples, we see how the prior assumptions and computational complexities of each method affects its application in reducing the dimensionality of certain datasets.</p>

- <em>The split Bregman method for total variation denoising</em>
  <br/>
  <a title="Amirkabir University of Technology" href="http://aut.ac.ir/" target="_blank">AUT</a>, May 30, 2010.
  <br/>
  [<a href="/files/the_split_bregman_method_for_l1_regularized_problems.pdf" target="_blank">slides</a>]
  <p><i>Details.</i> This is an overview of the split Bregman method for solving an $\ell_1$-regularized problem arising from TV denoising.</p>

### ML publications

- <em>Efficient classification based on sparse regression</em>
  <br/>
  MSc Thesis, Amirkabir University of Technology, July 2012.
  <br/>
  [<a title="Efficient classification based on sparse regression" href="/files/Noorzad2012b.pdf" target="_blank">thesis</a>, <a href="/files/thesis_in_persian.pdf" target="_blank">translation</a>, <a href="/files/defense_slides.pdf" target="_blank">slides</a>]

- <em>Regression with sparse approximations of data</em>
  <br/>
  with <a href="https://www.linkedin.com/in/bosturm/" target="_blank">Bob L. Sturm</a>
  <br/>
  European Signal Processing Conference (EUSIPCO), 2012.
  <br/>
  [<a title="Regression with sparse approximations of data" href="/files/Noorzad2012a.pdf" target="_blank">paper</a>, <a href="http://vbn.aau.dk/files/71866593/poster.pdf" target="_blank">poster</a>]

- <em>On automatic music genre recognition by sparse representation classification using auditory temporal modulations</em>
  <br/>
  with <a href="https://www.linkedin.com/in/bosturm/" target="_blank">Bob L. Sturm</a>
  <br/>
  Computer Music Modeling and Retrieval: Lecture Notes in Computer Sciences (LNCS). Springer, 2012.
  <br/>
  [<a href="http://cmmr2012.eecs.qmul.ac.uk/sites/cmmr2012.eecs.qmul.ac.uk/files/pdf/papers/cmmr2012_submission_17.pdf" target="_blank">paper</a>]
